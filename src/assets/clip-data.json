{
  "name": "CLIP Model",
  "description": "CLIP (Contrastive Language-Image Pre-training) is a neural network developed by OpenAI that learns visual concepts from natural language supervision. It's trained on 400 million (image, text) pairs and can perform zero-shot prediction tasks without specific training for those tasks.",
  "properties": {
    "Developed by": "OpenAI",
    "Released": "January 2021",
    "Training data": "400 million (image, text) pairs",
    "Architecture": "Dual-encoder (image and text)"
  },
  "children": [
    { 
      "name": "B/32 Variant", 
      "description": "The B/32 variant of CLIP uses a Vision Transformer (ViT) architecture with 32×32 pixel patches. This is the most lightweight variant of CLIP with a good balance of performance and computational efficiency.",
      "properties": {
        "Total parameters": "~151M",
        "ImageNet accuracy": "63.3% (zero-shot)",
        "Image resolution": "224×224 pixels"
      },
      "children": [
        {
          "name": "Visual Encoder (B/32)",
          "description": "The visual encoder transforms images into high-dimensional feature vectors. The B/32 variant divides images into 32×32 pixel patches and processes them through a Vision Transformer architecture.",
          "properties": {
            "Architecture type": "Vision Transformer (ViT)",
            "Input": "224×224 pixel images divided into 49 patches of 32×32 pixels"
          },
          "children": [
            { 
              "name": "Parameters: 87.85M",
              "description": "The visual encoder contains 87.85 million trainable parameters, which are adjusted during the training process to optimize the model's performance."
            },
            { 
              "name": "Hidden Size: 768",
              "description": "The hidden size refers to the dimension of the feature vectors within the transformer layers. A larger hidden size allows for more complex representations but requires more computation."
            },
            { 
              "name": "Attention Heads: 12",
              "description": "Multi-head attention splits the representation into 12 different attention mechanisms, allowing the model to focus on different aspects of the input simultaneously."
            },
            { 
              "name": "Layers: 12",
              "description": "The visual encoder consists of 12 transformer layers stacked on top of each other, each processing the output of the previous layer to create increasingly abstract representations."
            },
            { 
              "name": "Patch Size: 32×32",
              "description": "Images are divided into non-overlapping patches of 32×32 pixels. Each patch is treated as a token and linearly projected into the embedding space."
            },
            { 
              "name": "Image Resolution: 224px",
              "description": "The model processes square images with a resolution of 224×224 pixels, which is a standard size for many computer vision models."
            },
            { 
              "name": "Output Dimension: 512",
              "description": "The final output of the visual encoder is projected to a 512-dimensional vector, which is the same dimension as the text encoder output for compatibility."
            },
            {
              "name": "VisionTransformer",
              "description": "The Vision Transformer (ViT) architecture treats image patches similar to how transformers process word tokens, applying self-attention mechanisms to capture relationships between different parts of the image.",
              "children": [
                { 
                  "name": "Patch Embedding (Conv2d, 32×32)",
                  "description": "A convolutional layer that transforms each 32×32 image patch into a vector embedding. This is the first step in processing the image."
                },
                { 
                  "name": "Class Token (learnable)",
                  "description": "A special learnable token added to the sequence of patch embeddings. The final representation of this token is used as the image representation for classification."
                },
                { 
                  "name": "Positional Embedding (learnable)",
                  "description": "Learnable embeddings added to the patch embeddings to provide information about the spatial position of each patch in the original image."
                },
                { 
                  "name": "LayerNorm (Pre)",
                  "description": "Layer normalization applied before the transformer blocks to stabilize the input distribution and improve training dynamics."
                },
                { 
                  "name": "Transformer Blocks",
                  "description": "A series of transformer blocks that process the sequence of patch embeddings, capturing both local and global relationships in the image.",
                  "children": [
                    {
                      "name": "ResidualAttentionBlock",
                      "description": "A block combining self-attention and feed-forward layers with residual connections, allowing information to flow through the network more effectively.",
                      "children": [
                        { 
                          "name": "Self-Attention",
                          "description": "The self-attention mechanism allows each patch to attend to all other patches, capturing relationships between different parts of the image.",
                          "children": [
                            { 
                              "name": "LayerNorm",
                              "description": "Normalizes the input to the self-attention layer, stabilizing training and improving convergence."
                            },
                            { 
                              "name": "Multi-Head Attention (12 heads)",
                              "description": "Splits attention into 12 heads, allowing the model to focus on different aspects of the relationships between patches simultaneously."
                            },
                            { 
                              "name": "Residual Connection",
                              "description": "Adds the input to the output of the attention layer, helping with gradient flow during training and enabling deeper networks."
                            }
                          ]
                        },
                        {
                          "name": "MLP",
                          "description": "A multi-layer perceptron that processes each patch representation independently, adding non-linearity and increasing the model's capacity.",
                          "children": [
                            { 
                              "name": "LayerNorm",
                              "description": "Normalizes the input to the MLP, improving training stability."
                            },
                            { 
                              "name": "Linear (768→3072)",
                              "description": "A linear transformation that expands the dimension from 768 to 3072, increasing the model's capacity to represent complex patterns."
                            },
                            { 
                              "name": "QuickGELU Activation (x*sigmoid(1.702*x))",
                              "description": "A computationally efficient approximation of the GELU activation function, introducing non-linearity into the network."
                            },
                            { 
                              "name": "Linear (3072→768)",
                              "description": "A linear transformation that reduces the dimension back to 768, compressing the information processed by the expanded representation."
                            },
                            { 
                              "name": "Residual Connection",
                              "description": "Adds the input to the output of the MLP, helping with gradient flow and enabling deeper networks."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                { 
                  "name": "LayerNorm (Post)",
                  "description": "Layer normalization applied after the transformer blocks to normalize the output before the final projection."
                },
                { 
                  "name": "Projection Layer (768→512)",
                  "description": "A linear projection that reduces the dimension from 768 to 512, matching the dimension of the text encoder output for the shared embedding space."
                }
              ]
            }
          ]
        },
        {
          "name": "Text Encoder (B/32)",
          "description": "The text encoder transforms text inputs into high-dimensional feature vectors. It uses a transformer architecture similar to GPT but with bidirectional attention.",
          "properties": {
            "Architecture type": "Transformer",
            "Input": "Tokenized text with maximum 77 tokens"
          },
          "children": [
            { 
              "name": "Parameters: 63.43M",
              "description": "The text encoder contains 63.43 million trainable parameters, which are adjusted during training to optimize the model's performance."
            },
            { 
              "name": "Hidden Size: 512",
              "description": "The dimension of the feature vectors within the transformer layers. This determines the model's capacity to represent textual information."
            },
            { 
              "name": "Attention Heads: 8",
              "description": "Multi-head attention splits the representation into 8 different attention mechanisms, allowing the model to focus on different aspects of the text simultaneously."
            },
            { 
              "name": "Layers: 12",
              "description": "The text encoder consists of 12 transformer layers stacked on top of each other, processing the output of the previous layer to create increasingly abstract representations."
            },
            { 
              "name": "Context Length: 77",
              "description": "The maximum number of tokens that can be processed in a single forward pass. This limits the length of text that can be encoded."
            },
            { 
              "name": "Output Dimension: 512",
              "description": "The final output of the text encoder is a 512-dimensional vector, matching the dimension of the visual encoder output for the shared embedding space."
            },
            { 
              "name": "Token Embedding (vocab_size→512)",
              "description": "Converts each token (word or subword) into a 512-dimensional vector representation. The vocabulary size is typically around 49,152 tokens."
            },
            { 
              "name": "Positional Embedding (context_length×512)",
              "description": "Learnable embeddings added to the token embeddings to provide information about the position of each token in the sequence."
            },
            {
              "name": "Transformer",
              "description": "A series of transformer blocks that process the sequence of token embeddings, capturing the semantic meaning of the text.",
              "children": [
                {
                  "name": "ResidualAttentionBlocks",
                  "description": "Blocks combining self-attention and feed-forward layers with residual connections, allowing information to flow through the network more effectively.",
                  "children": [
                    {
                      "name": "Self-Attention",
                      "description": "The self-attention mechanism allows each token to attend to all other tokens, capturing relationships between different parts of the text.",
                      "children": [
                        { 
                          "name": "LayerNorm",
                          "description": "Normalizes the input to the self-attention layer, stabilizing training and improving convergence."
                        },
                        { 
                          "name": "Causal Multi-Head Attention (8 heads)",
                          "description": "A masked attention mechanism that prevents tokens from attending to future tokens, preserving the autoregressive property of the model."
                        },
                        { 
                          "name": "Residual Connection",
                          "description": "Adds the input to the output of the attention layer, helping with gradient flow during training and enabling deeper networks."
                        }
                      ]
                    },
                    {
                      "name": "MLP",
                      "description": "A multi-layer perceptron that processes each token representation independently, adding non-linearity and increasing the model's capacity.",
                      "children": [
                        { 
                          "name": "LayerNorm",
                          "description": "Normalizes the input to the MLP, improving training stability."
                        },
                        { 
                          "name": "Linear (512→2048)",
                          "description": "A linear transformation that expands the dimension from 512 to 2048, increasing the model's capacity to represent complex patterns."
                        },
                        { 
                          "name": "QuickGELU Activation (x*sigmoid(1.702*x))",
                          "description": "A computationally efficient approximation of the GELU activation function, introducing non-linearity into the network."
                        },
                        { 
                          "name": "Linear (2048→512)",
                          "description": "A linear transformation that reduces the dimension back to 512, compressing the information processed by the expanded representation."
                        },
                        { 
                          "name": "Residual Connection",
                          "description": "Adds the input to the output of the MLP, helping with gradient flow and enabling deeper networks."
                        }
                      ]
                    }
                  ]
                }
              ]
            },
            { 
              "name": "LayerNorm (Final)",
              "description": "Layer normalization applied to the output of the transformer blocks to normalize the final token representations."
            },
            { 
              "name": "Text Projection (512→512)",
              "description": "A linear projection that maps the text representation to the shared embedding space with the visual encoder."
            }
          ]
        },
        {
          "name": "Multimodal Features",
          "description": "The multimodal features component processes the outputs from both encoders to compute similarity between image and text representations in the shared embedding space.",
          "children": [
            { 
              "name": "Feature Normalization (L2 Norm)",
              "description": "L2 normalization ensures that all feature vectors have unit length, making cosine similarity calculations more stable and meaningful."
            },
            { 
              "name": "Logit Scale (learnable temperature parameter)",
              "description": "A learnable parameter that scales the dot products between image and text features, controlling the sharpness of the similarity distribution."
            },
            { 
              "name": "Cosine Similarity",
              "description": "Computes the cosine of the angle between image and text feature vectors, providing a measure of semantic similarity between them."
            }
          ]
        }
      ]
    },
    { 
      "name": "B/16 Variant", 
      "description": "The B/16 variant of CLIP uses a Vision Transformer with smaller 16×16 pixel patches, allowing for more detailed feature extraction compared to B/32, at the cost of increased computation.",
      "properties": {
        "Total parameters": "~177M",
        "ImageNet accuracy": "68.1% (zero-shot)",
        "Image resolution": "224×224 pixels"
      },
      "children": [
        {
          "name": "Visual Encoder (B/16)",
          "description": "The visual encoder for the B/16 variant processes images by dividing them into smaller 16×16 pixel patches, providing more granular visual information than the B/32 variant.",
          "properties": {
            "Architecture type": "Vision Transformer (ViT)",
            "Input": "224×224 pixel images divided into 196 patches of 16×16 pixels"
          },
          "children": [
            { 
              "name": "Parameters: 86.19M",
              "description": "The visual encoder contains 86.19 million trainable parameters, which are adjusted during training to optimize the model's performance."
            },
            { 
              "name": "Hidden Size: 768",
              "description": "The dimension of the feature vectors within the transformer layers, determining the model's capacity to represent visual information."
            },
            { 
              "name": "Attention Heads: 12",
              "description": "Multi-head attention splits the representation into 12 different attention mechanisms, allowing the model to focus on different aspects of the input simultaneously."
            },
            { 
              "name": "Layers: 12",
              "description": "The visual encoder consists of 12 transformer layers stacked on top of each other, processing the output of the previous layer to create increasingly abstract representations."
            },
            { 
              "name": "Patch Size: 16×16",
              "description": "Images are divided into non-overlapping patches of 16×16 pixels. The smaller patch size allows for more detailed feature extraction compared to the B/32 variant."
            },
            { 
              "name": "Image Resolution: 224px",
              "description": "The model processes square images with a resolution of 224×224 pixels, which is a standard size for many computer vision models."
            },
            { 
              "name": "Output Dimension: 512",
              "description": "The final output of the visual encoder is projected to a 512-dimensional vector, matching the dimension of the text encoder output for the shared embedding space."
            },
            {
              "name": "VisionTransformer",
              "description": "The Vision Transformer architecture treats image patches similar to how transformers process word tokens, applying self-attention mechanisms to capture relationships between different parts of the image.",
              "children": [
                { 
                  "name": "Patch Embedding (Conv2d, 16×16)",
                  "description": "A convolutional layer that transforms each 16×16 image patch into a vector embedding. This is the first step in processing the image."
                },
                { 
                  "name": "Class Token (learnable)",
                  "description": "A special learnable token added to the sequence of patch embeddings. The final representation of this token is used as the image representation for classification."
                },
                { 
                  "name": "Positional Embedding (learnable)",
                  "description": "Learnable embeddings added to the patch embeddings to provide information about the spatial position of each patch in the original image."
                },
                { 
                  "name": "LayerNorm (Pre)",
                  "description": "Layer normalization applied before the transformer blocks to stabilize the input distribution and improve training dynamics."
                },
                { 
                  "name": "Transformer Blocks",
                  "description": "A series of transformer blocks that process the sequence of patch embeddings, capturing both local and global relationships in the image.",
                  "children": [
                    {
                      "name": "ResidualAttentionBlock",
                      "description": "A block combining self-attention and feed-forward layers with residual connections, allowing information to flow through the network more effectively.",
                      "children": [
                        { 
                          "name": "Self-Attention",
                          "description": "The self-attention mechanism allows each patch to attend to all other patches, capturing relationships between different parts of the image.",
                          "children": [
                            { 
                              "name": "LayerNorm",
                              "description": "Normalizes the input to the self-attention layer, stabilizing training and improving convergence."
                            },
                            { 
                              "name": "Multi-Head Attention (12 heads)",
                              "description": "Splits attention into 12 heads, allowing the model to focus on different aspects of the relationships between patches simultaneously."
                            },
                            { 
                              "name": "Residual Connection",
                              "description": "Adds the input to the output of the attention layer, helping with gradient flow during training and enabling deeper networks."
                            }
                          ]
                        },
                        {
                          "name": "MLP",
                          "description": "A multi-layer perceptron that processes each patch representation independently, adding non-linearity and increasing the model's capacity.",
                          "children": [
                            { 
                              "name": "LayerNorm",
                              "description": "Normalizes the input to the MLP, improving training stability."
                            },
                            { 
                              "name": "Linear (768→3072)",
                              "description": "A linear transformation that expands the dimension from 768 to 3072, increasing the model's capacity to represent complex patterns."
                            },
                            { 
                              "name": "QuickGELU Activation (x*sigmoid(1.702*x))",
                              "description": "A computationally efficient approximation of the GELU activation function, introducing non-linearity into the network."
                            },
                            { 
                              "name": "Linear (3072→768)",
                              "description": "A linear transformation that reduces the dimension back to 768, compressing the information processed by the expanded representation."
                            },
                            { 
                              "name": "Residual Connection",
                              "description": "Adds the input to the output of the MLP, helping with gradient flow and enabling deeper networks."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                { 
                  "name": "LayerNorm (Post)",
                  "description": "Layer normalization applied after the transformer blocks to normalize the output before the final projection."
                },
                { 
                  "name": "Projection Layer (768→512)",
                  "description": "A linear projection that reduces the dimension from 768 to 512, matching the dimension of the text encoder output for the shared embedding space."
                }
              ]
            }
          ]
        },
        {
          "name": "Text Encoder (B/16)",
          "description": "The text encoder for the B/16 variant transforms text inputs into feature vectors using a transformer architecture. It's similar to the B/32 text encoder but with different parameter counts.",
          "properties": {
            "Architecture type": "Transformer",
            "Input": "Tokenized text with maximum 77 tokens"
          },
          "children": [
            { 
              "name": "Parameters: 91.16M",
              "description": "The text encoder contains 91.16 million trainable parameters, which are adjusted during training to optimize the model's performance."
            },
            { 
              "name": "Hidden Size: 512",
              "description": "The dimension of the feature vectors within the transformer layers, determining the model's capacity to represent textual information."
            },
            { 
              "name": "Attention Heads: 8",
              "description": "Multi-head attention splits the representation into 8 different attention mechanisms, allowing the model to focus on different aspects of the text simultaneously."
            },
            { 
              "name": "Layers: 12",
              "description": "The text encoder consists of 12 transformer layers stacked on top of each other, processing the output of the previous layer to create increasingly abstract representations."
            },
            { 
              "name": "Context Length: 77",
              "description": "The maximum number of tokens that can be processed in a single forward pass. This limits the length of text that can be encoded."
            },
            { 
              "name": "Output Dimension: 512",
              "description": "The final output of the text encoder is a 512-dimensional vector, matching the dimension of the visual encoder output for the shared embedding space."
            },
            { 
              "name": "Token Embedding (vocab_size→512)",
              "description": "Converts each token (word or subword) into a 512-dimensional vector representation. The vocabulary size is typically around 49,152 tokens."
            },
            { 
              "name": "Positional Embedding (context_length×512)",
              "description": "Learnable embeddings added to the token embeddings to provide information about the position of each token in the sequence."
            },
            {
              "name": "Transformer",
              "description": "A series of transformer blocks that process the sequence of token embeddings, capturing the semantic meaning of the text.",
              "children": [
                {
                  "name": "ResidualAttentionBlocks",
                  "description": "Blocks combining self-attention and feed-forward layers with residual connections, allowing information to flow through the network more effectively.",
                  "children": [
                    {
                      "name": "Self-Attention",
                      "description": "The self-attention mechanism allows each token to attend to all other tokens, capturing relationships between different parts of the text.",
                      "children": [
                        { 
                          "name": "LayerNorm",
                          "description": "Normalizes the input to the self-attention layer, stabilizing training and improving convergence."
                        },
                        { 
                          "name": "Causal Multi-Head Attention (8 heads)",
                          "description": "A masked attention mechanism that prevents tokens from attending to future tokens, preserving the autoregressive property of the model."
                        },
                        { 
                          "name": "Residual Connection",
                          "description": "Adds the input to the output of the attention layer, helping with gradient flow during training and enabling deeper networks."
                        }
                      ]
                    },
                    {
                      "name": "MLP",
                      "description": "A multi-layer perceptron that processes each token representation independently, adding non-linearity and increasing the model's capacity.",
                      "children": [
                        { 
                          "name": "LayerNorm",
                          "description": "Normalizes the input to the MLP, improving training stability."
                        },
                        { 
                          "name": "Linear (512→2048)",
                          "description": "A linear transformation that expands the dimension from 512 to 2048, increasing the model's capacity to represent complex patterns."
                        },
                        { 
                          "name": "QuickGELU Activation (x*sigmoid(1.702*x))",
                          "description": "A computationally efficient approximation of the GELU activation function, introducing non-linearity into the network."
                        },
                        { 
                          "name": "Linear (2048→512)",
                          "description": "A linear transformation that reduces the dimension back to 512, compressing the information processed by the expanded representation."
                        },
                        { 
                          "name": "Residual Connection",
                          "description": "Adds the input to the output of the MLP, helping with gradient flow and enabling deeper networks."
                        }
                      ]
                    }
                  ]
                }
              ]
            },
            { 
              "name": "LayerNorm (Final)",
              "description": "Layer normalization applied to the output of the transformer blocks to normalize the final token representations."
            },
            { 
              "name": "Text Projection (512→512)",
              "description": "A linear projection that maps the text representation to the shared embedding space with the visual encoder."
            }
          ]
        },
        {
          "name": "Multimodal Features",
          "description": "The multimodal features component processes the outputs from both encoders to compute similarity between image and text representations in the shared embedding space.",
          "children": [
            { 
              "name": "Feature Normalization (L2 Norm)",
              "description": "L2 normalization ensures that all feature vectors have unit length, making cosine similarity calculations more stable and meaningful."
            },
            { 
              "name": "Logit Scale (learnable temperature parameter)",
              "description": "A learnable parameter that scales the dot products between image and text features, controlling the sharpness of the similarity distribution."
            },
            { 
              "name": "Cosine Similarity",
              "description": "Computes the cosine of the angle between image and text feature vectors, providing a measure of semantic similarity between them."
            }
          ]
        }
      ]
    },
    { 
      "name": "L/14 Variant", 
      "description": "The L/14 variant is the largest standard CLIP model, using a Vision Transformer with 14×14 pixel patches and significantly more parameters. It achieves the highest accuracy among the CLIP variants.",
      "properties": {
        "Total parameters": "~428M",
        "ImageNet accuracy": "75.4% (zero-shot)",
        "Image resolution": "224×224 pixels"
      },
      "children": [
        {
          "name": "Visual Encoder (L/14)",
          "description": "The visual encoder for the L/14 variant processes images using a large Vision Transformer with 14×14 pixel patches, providing the most detailed feature extraction among standard CLIP variants.",
          "properties": {
            "Architecture type": "Vision Transformer (ViT-L)",
            "Input": "224×224 pixel images divided into 256 patches of 14×14 pixels"
          },
          "children": [
            { 
              "name": "Parameters: 303.97M",
              "description": "The visual encoder contains 303.97 million trainable parameters, significantly more than the B/32 and B/16 variants, allowing for more complex representations."
            },
            { 
              "name": "Hidden Size: 1024",
              "description": "The dimension of the feature vectors within the transformer layers is 1024, larger than the B variants, providing greater representational capacity."
            },
            { 
              "name": "Attention Heads: 16",
              "description": "Multi-head attention splits the representation into 16 different attention mechanisms, allowing for more fine-grained attention to different aspects of the input."
            },
            { 
              "name": "Layers: 24",
              "description": "The visual encoder consists of 24 transformer layers, twice as many as the B variants, allowing for more complex hierarchical processing of visual information."
            },
            { 
              "name": "Patch Size: 14×14",
              "description": "Images are divided into non-overlapping patches of 14×14 pixels, smaller than both B variants, allowing for even more detailed feature extraction."
            },
            { 
              "name": "Image Resolution: 224px",
              "description": "The model processes square images with a resolution of 224×224 pixels, which is a standard size for many computer vision models."
            },
            { 
              "name": "Output Dimension: 768",
              "description": "The final output of the visual encoder is projected to a 768-dimensional vector, larger than the B variants and matching the dimension of the L/14 text encoder output."
            },
            {
              "name": "VisionTransformer",
              "description": "The Vision Transformer architecture treats image patches similar to how transformers process word tokens, applying self-attention mechanisms to capture relationships between different parts of the image.",
              "children": [
                { 
                  "name": "Patch Embedding (Conv2d, 14×14)",
                  "description": "A convolutional layer that transforms each 14×14 image patch into a vector embedding. This is the first step in processing the image."
                },
                { 
                  "name": "Class Token (learnable)",
                  "description": "A special learnable token added to the sequence of patch embeddings. The final representation of this token is used as the image representation for classification."
                },
                { 
                  "name": "Positional Embedding (learnable)",
                  "description": "Learnable embeddings added to the patch embeddings to provide information about the spatial position of each patch in the original image."
                },
                { 
                  "name": "LayerNorm (Pre)",
                  "description": "Layer normalization applied before the transformer blocks to stabilize the input distribution and improve training dynamics."
                },
                { 
                  "name": "Transformer Blocks",
                  "description": "A series of transformer blocks that process the sequence of patch embeddings, capturing both local and global relationships in the image.",
                  "children": [
                    {
                      "name": "ResidualAttentionBlock",
                      "description": "A block combining self-attention and feed-forward layers with residual connections, allowing information to flow through the network more effectively.",
                      "children": [
                        { 
                          "name": "Self-Attention",
                          "description": "The self-attention mechanism allows each patch to attend to all other patches, capturing relationships between different parts of the image.",
                          "children": [
                            { 
                              "name": "LayerNorm",
                              "description": "Normalizes the input to the self-attention layer, stabilizing training and improving convergence."
                            },
                            { 
                              "name": "Multi-Head Attention (16 heads)",
                              "description": "Splits attention into 16 heads, allowing the model to focus on different aspects of the relationships between patches simultaneously."
                            },
                            { 
                              "name": "Residual Connection",
                              "description": "Adds the input to the output of the attention layer, helping with gradient flow during training and enabling deeper networks."
                            }
                          ]
                        },
                        {
                          "name": "MLP",
                          "description": "A multi-layer perceptron that processes each patch representation independently, adding non-linearity and increasing the model's capacity.",
                          "children": [
                            { 
                              "name": "LayerNorm",
                              "description": "Normalizes the input to the MLP, improving training stability."
                            },
                            { 
                              "name": "Linear (1024→4096)",
                              "description": "A linear transformation that expands the dimension from 1024 to 4096, increasing the model's capacity to represent complex patterns."
                            },
                            { 
                              "name": "QuickGELU Activation (x*sigmoid(1.702*x))",
                              "description": "A computationally efficient approximation of the GELU activation function, introducing non-linearity into the network."
                            },
                            { 
                              "name": "Linear (4096→1024)",
                              "description": "A linear transformation that reduces the dimension back to 1024, compressing the information processed by the expanded representation."
                            },
                            { 
                              "name": "Residual Connection",
                              "description": "Adds the input to the output of the MLP, helping with gradient flow and enabling deeper networks."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                { 
                  "name": "LayerNorm (Post)",
                  "description": "Layer normalization applied after the transformer blocks to normalize the output before the final projection."
                },
                { 
                  "name": "Projection Layer (1024→768)",
                  "description": "A linear projection that reduces the dimension from 1024 to 768, matching the dimension of the text encoder output for the shared embedding space."
                }
              ]
            }
          ]
        },
        {
          "name": "Text Encoder (L/14)",
          "description": "The text encoder for the L/14 variant transforms text inputs into feature vectors using a larger transformer architecture than the B variants, with a higher hidden dimension.",
          "properties": {
            "Architecture type": "Transformer",
            "Input": "Tokenized text with maximum 77 tokens"
          },
          "children": [
            { 
              "name": "Parameters: 123.65M",
              "description": "The text encoder contains 123.65 million trainable parameters, more than the B variants, allowing for more complex representations of text."
            },
            { 
              "name": "Hidden Size: 768",
              "description": "The dimension of the feature vectors within the transformer layers is 768, larger than the B variants, providing greater representational capacity."
            },
            { 
              "name": "Attention Heads: 12",
              "description": "Multi-head attention splits the representation into 12 different attention mechanisms, allowing the model to focus on different aspects of the text simultaneously."
            },
            { 
              "name": "Layers: 12",
              "description": "The text encoder consists of 12 transformer layers stacked on top of each other, processing the output of the previous layer to create increasingly abstract representations."
            },
            { 
              "name": "Context Length: 77",
              "description": "The maximum number of tokens that can be processed in a single forward pass. This limits the length of text that can be encoded."
            },
            { 
              "name": "Output Dimension: 768",
              "description": "The final output of the text encoder is a 768-dimensional vector, matching the dimension of the visual encoder output for the shared embedding space."
            },
            { 
              "name": "Token Embedding (vocab_size→768)",
              "description": "Converts each token (word or subword) into a 768-dimensional vector representation. The vocabulary size is typically around 49,152 tokens."
            },
            { 
              "name": "Positional Embedding (context_length×768)",
              "description": "Learnable embeddings added to the token embeddings to provide information about the position of each token in the sequence."
            },
            {
              "name": "Transformer",
              "description": "A series of transformer blocks that process the sequence of token embeddings, capturing the semantic meaning of the text.",
              "children": [
                {
                  "name": "ResidualAttentionBlocks",
                  "description": "Blocks combining self-attention and feed-forward layers with residual connections, allowing information to flow through the network more effectively.",
                  "children": [
                    {
                      "name": "Self-Attention",
                      "description": "The self-attention mechanism allows each token to attend to all other tokens, capturing relationships between different parts of the text.",
                      "children": [
                        { 
                          "name": "LayerNorm",
                          "description": "Normalizes the input to the self-attention layer, stabilizing training and improving convergence."
                        },
                        { 
                          "name": "Causal Multi-Head Attention (12 heads)",
                          "description": "A masked attention mechanism that prevents tokens from attending to future tokens, preserving the autoregressive property of the model."
                        },
                        { 
                          "name": "Residual Connection",
                          "description": "Adds the input to the output of the attention layer, helping with gradient flow during training and enabling deeper networks."
                        }
                      ]
                    },
                    {
                      "name": "MLP",
                      "description": "A multi-layer perceptron that processes each token representation independently, adding non-linearity and increasing the model's capacity.",
                      "children": [
                        { 
                          "name": "LayerNorm",
                          "description": "Normalizes the input to the MLP, improving training stability."
                        },
                        { 
                          "name": "Linear (768→3072)",
                          "description": "A linear transformation that expands the dimension from 768 to 3072, increasing the model's capacity to represent complex patterns."
                        },
                        { 
                          "name": "QuickGELU Activation (x*sigmoid(1.702*x))",
                          "description": "A computationally efficient approximation of the GELU activation function, introducing non-linearity into the network."
                        },
                        { 
                          "name": "Linear (3072→768)",
                          "description": "A linear transformation that reduces the dimension back to 768, compressing the information processed by the expanded representation."
                        },
                        { 
                          "name": "Residual Connection",
                          "description": "Adds the input to the output of the MLP, helping with gradient flow and enabling deeper networks."
                        }
                      ]
                    }
                  ]
                }
              ]
            },
            { 
              "name": "LayerNorm (Final)",
              "description": "Layer normalization applied to the output of the transformer blocks to normalize the final token representations."
            },
            { 
              "name": "Text Projection (768→768)",
              "description": "A linear projection that maps the text representation to the shared embedding space with the visual encoder."
            }
          ]
        },
        {
          "name": "Multimodal Features",
          "description": "The multimodal features component processes the outputs from both encoders to compute similarity between image and text representations in the shared embedding space.",
          "children": [
            { 
              "name": "Feature Normalization (L2 Norm)",
              "description": "L2 normalization ensures that all feature vectors have unit length, making cosine similarity calculations more stable and meaningful."
            },
            { 
              "name": "Logit Scale (learnable temperature parameter)",
              "description": "A learnable parameter that scales the dot products between image and text features, controlling the sharpness of the similarity distribution."
            },
            { 
              "name": "Cosine Similarity",
              "description": "Computes the cosine of the angle between image and text feature vectors, providing a measure of semantic similarity between them."
            }
          ]
        }
      ]
    },
    {
      "name": "Forward Pass Flow",
      "description": "The forward pass flow describes how data moves through the CLIP model during inference, from raw inputs to final similarity scores between image and text.",
      "properties": {
        "Input types": "Images and text",
        "Output": "Similarity scores between image-text pairs",
        "Processing": "Parallel encoding followed by similarity computation"
      },
      "children": [
        { 
          "name": "Image Encoding",
          "description": "The process of transforming a raw image into a high-dimensional feature vector using the visual encoder.",
          "children": [
            { 
              "name": "Input: Raw image",
              "description": "The raw image input, typically resized to 224×224 pixels and normalized according to the model's preprocessing requirements."
            },
            { 
              "name": "Process through Visual Encoder",
              "description": "The image is divided into patches, embedded, and processed through the Vision Transformer to extract visual features."
            },
            { 
              "name": "Output: image_features",
              "description": "The final output is a high-dimensional vector (512 or 768 dimensions depending on the variant) representing the semantic content of the image."
            }
          ]
        },
        { 
          "name": "Text Encoding",
          "description": "The process of transforming raw text into a high-dimensional feature vector using the text encoder.",
          "children": [
            { 
              "name": "Input: Tokenized text",
              "description": "The text input is tokenized into a sequence of tokens using a byte-pair encoding (BPE) tokenizer with a vocabulary size of around 49,152."
            },
            { 
              "name": "Process through Text Encoder",
              "description": "The tokenized text is embedded and processed through the transformer to extract semantic features."
            },
            { 
              "name": "Output: text_features",
              "description": "The final output is a high-dimensional vector (512 or 768 dimensions depending on the variant) representing the semantic content of the text."
            }
          ]
        },
        { 
          "name": "Feature Normalization",
          "description": "The process of normalizing the feature vectors to have unit length, which is important for computing meaningful cosine similarities.",
          "children": [
            { 
              "name": "L2 normalize image_features",
              "description": "The image feature vector is divided by its L2 norm (Euclidean length) to create a unit vector, ensuring consistent similarity calculations."
            },
            { 
              "name": "L2 normalize text_features",
              "description": "The text feature vector is divided by its L2 norm to create a unit vector, ensuring consistent similarity calculations."
            }
          ]
        },
        { 
          "name": "Similarity Calculation",
          "description": "The process of computing the similarity between image and text features to determine how well they match semantically.",
          "children": [
            { 
              "name": "Scale by exp(logit_scale)",
              "description": "The similarity scores are scaled by a learned temperature parameter to control the sharpness of the similarity distribution."
            },
            { 
              "name": "Compute cosine similarity: image_features @ text_features.t()",
              "description": "The dot product between normalized image and text features computes the cosine similarity, measuring how aligned they are in the shared embedding space."
            },
            { 
              "name": "Output: logits_per_image, logits_per_text",
              "description": "The final outputs are matrices of similarity scores between each image and all texts (logits_per_image) and between each text and all images (logits_per_text)."
            }
          ]
        }
      ]
    },
    {
      "name": "Training Objective",
      "description": "The training objective defines how CLIP learns to align image and text representations in the shared embedding space through contrastive learning.",
      "properties": {
        "Dataset size": "400 million (image, text) pairs",
        "Training strategy": "Contrastive learning",
        "Optimization": "AdamW optimizer with cosine learning rate schedule"
      },
      "children": [
        { 
          "name": "Contrastive Loss (InfoNCE)",
          "description": "A loss function that encourages the model to maximize the similarity between matching image-text pairs while minimizing it for non-matching pairs. It's based on the InfoNCE (Noise Contrastive Estimation) loss."
        },
        { 
          "name": "Temperature Parameter (logit_scale)",
          "description": "A learnable parameter that scales the similarity scores, controlling the sharpness of the similarity distribution. It's parameterized as exp(logit_scale) to ensure it's always positive."
        },
        { 
          "name": "Symmetric Cross Entropy",
          "description": "The loss is computed symmetrically for both image-to-text and text-to-image directions, ensuring that the model learns bidirectional associations between modalities."
        }
      ]
    }
  ]
}
